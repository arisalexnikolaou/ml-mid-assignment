{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efd4b93",
   "metadata": {},
   "source": [
    " Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90f5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np  # type: ignore\n",
    "from typing import Any\n",
    "import pandas as pd  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore MinMaxScaler performed more poorly\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # type: ignore\n",
    "from tensorflow.keras.models import Sequential  # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout  # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # type: ignore\n",
    "# Cell: GPU diagnostics\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "RANDOM_SEED: int = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "TARGET_COLUMN: str = \"MeanTemp\"  # the column to predict\n",
    "\n",
    "LOOKBACK: int = 14  # use last x days to predict next day\n",
    "PATIENCE: int = 3  # early stopping patience\n",
    "EPOCHS: int = 50  # maximum number of training epochs\n",
    "BATCH_SIZE: int = 16  # training batch size, reducing batch size improves accuracy but increases training time\n",
    "DENSE_LAYER: int = 64  # number of neurons in dense layer\n",
    "ACTIVATION: str = \"relu\"  # activation function for dense layer\n",
    "LSTM_UNITS: int = 128\n",
    "DROPOUT_RATE: float = 0.20  # dropout rate for regularization\n",
    "TRAIN_RATIO: float = 0.70  # proportion of data to use for training\n",
    "STA_WEIGHT: float = 2.0\n",
    "\n",
    "# scaler to use for data normalization\n",
    "# scaler: MinMaxScaler = MinMaxScaler(feature_range=(0, 1)) # performs worse than standard scaler\n",
    "standard_scaler: StandardScaler = StandardScaler()\n",
    "\n",
    "# ALL_FEATURES: List[str] = [\n",
    "#     \"STA\",  # station identifier, predictions should be grouped per station\n",
    "#     \"Precip\",\n",
    "#     \"WindGustSpd\",\n",
    "#     \"MaxTemp\",\n",
    "#     \"MinTemp\",\n",
    "#     \"MeanTemp\",\n",
    "#     \"Snowfall\",\n",
    "#     # 'PoorWeather',\n",
    "#     \"YR\",\n",
    "#     \"MO\",\n",
    "#     # 'DA',\n",
    "#     \"PRCP\",\n",
    "#     # 'DR', -- too many missing values\n",
    "#     # 'SPD', -- too many missing values\n",
    "#     # 'MAX', -- will sqew results\n",
    "#     # 'MIN', -- will sqew results\n",
    "#     # 'MEA', mean temperature in fahrenheit ignore\n",
    "#     # \"SNF\",\n",
    "#     # 'SND',\n",
    "#     # 'PGT'\n",
    "# ]\n",
    "\n",
    "STANDARD_FEATURES: List[str] = [\n",
    "    \"STA\",  # station identifier, predictions should be grouped per station\n",
    "    # \"WindGustSpd\",\n",
    "    \"MaxTemp\",\n",
    "    \"MinTemp\",\n",
    "    \"MeanTemp\",\n",
    "    # \"PGT\",\n",
    "    # \"YR\",\n",
    "    \"MO\",\n",
    "    # \"DA\",\n",
    "]\n",
    "\n",
    "ROBUST_FEATURES: List[str] = [\"Snowfall\", \"Precip\"]\n",
    "\n",
    "ALL_FEATURES = STANDARD_FEATURES + ROBUST_FEATURES\n",
    "\n",
    "FEATURE_COLUMNS: List[str] = [\n",
    "    'STA', # station identifier\n",
    "    'Precip', \n",
    "    'WindGustSpd', \n",
    "    'MaxTemp', \n",
    "    'MinTemp', \n",
    "    'MeanTemp', \n",
    "    'Snowfall', \n",
    "    # 'PoorWeather', \n",
    "    'YR', 'MO', \n",
    "    # 'DA', \n",
    "    'PRCP', \n",
    "    # 'DR', -- too many missing values\n",
    "    # 'SPD', \n",
    "    'MAX', \n",
    "    'MIN', \n",
    "    # 'MEA', mean temperature in fahrenheit ignore\n",
    "    'SNF', \n",
    "    # 'SND', \n",
    "    # 'PGT'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee3904",
   "metadata": {},
   "source": [
    "GPU Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234df4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "if gpus := tf.config.list_physical_devices(\"GPU\"):\n",
    "    print(f\"GPUs available: {gpus}\")\n",
    "    try:\n",
    "        # Optional: restrict TensorFlow to the first GPU\n",
    "        tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(f\"Using GPU: {logical_gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU config error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9658e1",
   "metadata": {},
   "source": [
    "### Load and inspect data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3499f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Summary of Weather.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m DATA_PATH: Path = Path(\u001b[33m\"\u001b[39m\u001b[33mSummary of Weather.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df: pd.DataFrame = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\mary\\ml-mid-assignment\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\mary\\ml-mid-assignment\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\mary\\ml-mid-assignment\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\mary\\ml-mid-assignment\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\mary\\ml-mid-assignment\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Summary of Weather.csv'"
     ]
    }
   ],
   "source": [
    "DATA_PATH: Path = Path(\"Summary of Weather.csv\")\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8e138",
   "metadata": {},
   "source": [
    "### Show basic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d42d0",
   "metadata": {},
   "source": [
    "### Drop columns with no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original columns:\", df.columns.tolist())\n",
    "print(len(df.columns))\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "print(\"Remaining columns:\", df.columns.tolist())\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc853162",
   "metadata": {},
   "source": [
    "### Fill columns with zero where no values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"PRCP\", \"Snowfall\", \"SNF\", \"Precip\"]] = df[[\"PRCP\", \"Snowfall\", \"SNF\", \"Precip\"]].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82270f43",
   "metadata": {},
   "source": [
    "### Drop meaningless columns that would sqew results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"SNF\", \"MAX\", \"MIN\", \"MEA\", \"PoorWeather\", \"TSHDSBRSGF\", \"PRCP\"]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9773b",
   "metadata": {},
   "source": [
    "### Drop duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97594dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicated columns: {df.duplicated().sum()}\")\n",
    "\n",
    "if df.duplicated().sum() > 0:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "int(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c8c40",
   "metadata": {},
   "source": [
    "### Process STA, Year, Month and DA as ints and fill missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9691812",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [\"STA\", \"YR\", \"MO\", \"DA\"]\n",
    "\n",
    "for col in int_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129a5c4",
   "metadata": {},
   "source": [
    "### Select features and basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only needed columns + Date for reference\n",
    "columns_to_use: list[str] = [\"Date\"] + ALL_FEATURES\n",
    "df_model: pd.DataFrame = df[columns_to_use].copy()\n",
    "\n",
    "# Handle 'T' in precipitation-like columns if they exist\n",
    "for col in [\"Precip\", \"PRCP\"]:\n",
    "    if col in df_model.columns:\n",
    "        if df_model[col].dtype in (\"str\", \"object\"):\n",
    "            df_model[col] = df_model[col].replace(\"T\", 0.0)\n",
    "        # Cast to float\n",
    "        df_model[col] = pd.to_numeric(df_model[col], errors=\"coerce\")\n",
    "\n",
    "# Convert any remaining object columns that look numeric\n",
    "for col in df_model.columns:\n",
    "    if df_model[col].dtype == \"object\":\n",
    "        # Try to coerce to numeric, keep non-numeric as NaN\n",
    "        df_model[col] = pd.to_numeric(df_model[col], errors=\"coerce\")\n",
    "\n",
    "# Parse date and sort\n",
    "df_model[\"Date\"] = pd.to_datetime(df_model[\"Date\"])\n",
    "df_model = df_model.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Drop rows with any missing selected feature values\n",
    "df_model = df_model.dropna(subset=ALL_FEATURES).reset_index(drop=True)\n",
    "df_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f624ec",
   "metadata": {},
   "source": [
    "### Build supervised sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(\n",
    "    data: np.ndarray, target_index: int, lookback: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create (X, y) sequences for LSTM from multivariate time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Time-series data of shape (n_samples, n_features).\n",
    "    target_index : int\n",
    "        Index of the feature column to be predicted.\n",
    "    lookback : int\n",
    "        Number of past time steps used as input.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        X of shape (n_sequences, lookback, n_features),\n",
    "        y of shape (n_sequences,).\n",
    "    \"\"\"\n",
    "    X_list: list[np.ndarray] = []\n",
    "    y_list: list[float] = []\n",
    "\n",
    "    for i in range(lookback, len(data)):\n",
    "        X_list.append(data[i - lookback : i])\n",
    "        y_list.append(data[i, target_index])\n",
    "\n",
    "    X: np.ndarray = np.array(X_list)\n",
    "    y: np.ndarray = np.array(y_list)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbc1e1",
   "metadata": {},
   "source": [
    "### Scaling and sequence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d09279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature values\n",
    "features: np.ndarray = df_model[ALL_FEATURES].values\n",
    "\n",
    "# scale to be able to train LSTM model\n",
    "scaled_features: np.ndarray = standard_scaler.fit_transform(features)\n",
    "\n",
    "# add more weight to STA feature\n",
    "scaled_features[:, 1] *= STA_WEIGHT  # assuming STA is the first column\n",
    "\n",
    "target_index: int = ALL_FEATURES.index(TARGET_COLUMN)\n",
    "\n",
    "X_all, y_all = create_sequences(\n",
    "    data=scaled_features, target_index=target_index, lookback=LOOKBACK\n",
    ")\n",
    "\n",
    "X_all.shape, y_all.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0b154",
   "metadata": {},
   "source": [
    " Train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(\n",
    "    X: np.ndarray, y: np.ndarray, train_ratio: float = 0.7, val_ratio: float = 0.15\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split time-series data into train, validation, and test partitions\n",
    "    preserving temporal order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input sequences of shape (n, lookback, n_features).\n",
    "    y : np.ndarray\n",
    "        Targets of shape (n,).\n",
    "    train_ratio : float\n",
    "        Fraction of samples for training.\n",
    "    val_ratio : float\n",
    "        Fraction of samples for validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of arrays\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test.\n",
    "    \"\"\"\n",
    "    n_samples: int = len(X)\n",
    "    train_end: int = int(n_samples * train_ratio)\n",
    "    val_end: int = int(n_samples * (train_ratio + val_ratio))\n",
    "\n",
    "    X_train = X_all[:train_end]\n",
    "    y_train = y_all[:train_end]\n",
    "\n",
    "    X_val = X_all[train_end:val_end]\n",
    "    y_val = y_all[train_end:val_end]\n",
    "\n",
    "    X_test = X_all[val_end:]\n",
    "    y_test = y_all[val_end:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(\n",
    "    X_all, y_all, train_ratio=TRAIN_RATIO\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ba30d",
   "metadata": {},
   "source": [
    "Define and train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(\n",
    "    input_shape: tuple[int, int],\n",
    "    lstm_units: int = 128,\n",
    "    dropout_rate: float = 0.5,\n",
    ") -> Sequential:\n",
    "    \"\"\"\n",
    "    Build a simple LSTM regression model in Keras.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : tuple[int, int]\n",
    "        Shape (lookback, n_features) for the input.\n",
    "    lstm_units : int\n",
    "        Number of LSTM units.\n",
    "    dropout_rate : float\n",
    "        Dropout rate after the LSTM layer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "        Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(DENSE_LAYER, activation=ACTIVATION))\n",
    "    model.add(Dense(1))  # regression output\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_shape: tuple[int, int] = (X_train.shape[1], X_train.shape[2])\n",
    "model: Sequential = build_lstm_model(\n",
    "    input_shape=input_shape, lstm_units=LSTM_UNITS, dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fb7f34",
   "metadata": {},
   "source": [
    "Train model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39243b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c81595",
   "metadata": {},
   "source": [
    "Visualize training loss and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history) -> None:\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and MAE over epochs.\n",
    "    \"\"\"\n",
    "    history_dict = history.history\n",
    "    epochs = range(1, len(history_dict[\"loss\"]) + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].plot(epochs, history_dict[\"loss\"], label=\"Train loss (MSE)\")\n",
    "    axes[0].plot(epochs, history_dict[\"val_loss\"], label=\"Val loss (MSE)\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training and validation loss MSE\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, history_dict[\"mae\"], label=\"Train MAE\")\n",
    "    axes[1].plot(epochs, history_dict[\"val_mae\"], label=\"Val MAE\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"MAE\")\n",
    "    axes[1].set_title(\"Training and validation MAE\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f6a06",
   "metadata": {},
   "source": [
    "Evaluate on test set and compute scores descale features to interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_scaled: np.ndarray = model.predict(X_test)\n",
    "\n",
    "\n",
    "# For interpretability, inverse-transform MeanTemp back to original units.\n",
    "# We need to embed predictions into a full feature vector before inverse scaling.\n",
    "def inverse_transform_target(\n",
    "    scaled_target: np.ndarray,\n",
    "    scaler: Any,\n",
    "    target_index: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverse-transform a single target column that was part of a scaler fit\n",
    "    on multiple features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scaled_target : np.ndarray\n",
    "        Predicted or true target values in scaled space, shape (n_samples, 1) or (n_samples,).\n",
    "    scaler : Any\n",
    "        Fitted scaler on the full feature matrix.\n",
    "    target_index : int\n",
    "        Index of the target column in the original feature matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Target values in original units, shape (n_samples,).\n",
    "    \"\"\"\n",
    "    scaled_target = np.asarray(scaled_target).reshape(-1, 1)\n",
    "    n_features: int = scaler.n_features_in_\n",
    "\n",
    "    # Create dummy zeros for all features, then replace target column\n",
    "    dummy_scaled = np.zeros((len(scaled_target), n_features))\n",
    "    dummy_scaled[:, target_index] = scaled_target[:, 0]\n",
    "\n",
    "    inv_full = scaler.inverse_transform(dummy_scaled)\n",
    "    return inv_full[:, target_index]\n",
    "\n",
    "\n",
    "y_test_inv: np.ndarray = inverse_transform_target(y_test, standard_scaler, target_index)\n",
    "y_pred_test_inv: np.ndarray = inverse_transform_target(\n",
    "    y_pred_test_scaled, standard_scaler, target_index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811483cd",
   "metadata": {},
   "source": [
    "Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "\n",
    "test_mse, test_rmse, test_mae, test_r2 = regression_metrics(y_test_inv, y_pred_test_inv)\n",
    "\n",
    "print(\"\\n=== Test metrics ===\")\n",
    "print(f\"MSE : {test_mse:.4f}\")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"MAE : {test_mae:.4f}\")\n",
    "print(f\"R²  : {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2a6f7",
   "metadata": {},
   "source": [
    "Predictions vs Actuals (LSTM Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    title: str = \"Actual vs predicted MeanTemp\",\n",
    "    n_points: int = 300,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values in original units.\n",
    "    y_pred : np.ndarray\n",
    "        Predicted target values in original units.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "    n_points : int\n",
    "        Number of last points to plot for readability.\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    start: int = max(0, n - n_points)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(range(start, n), y_true[start:], label=\"Actual\")\n",
    "    plt.plot(range(start, n), y_pred[start:], label=\"Predicted\")\n",
    "    plt.xlabel(\"Time index (relative)\")\n",
    "    plt.ylabel(\"MeanTemp\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_predictions(y_test_inv, y_pred_test_inv, n_points=150)\n",
    "\n",
    "print(len(y_pred_test_inv))\n",
    "print(len(y_test_inv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c89f6",
   "metadata": {},
   "source": [
    "### Residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals: np.ndarray = y_test_inv - y_pred_test_inv\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(residuals)\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.xlabel(\"Time index (test set)\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residuals over time\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756f1a7",
   "metadata": {},
   "source": [
    "Linear Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a333ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_model.dropna(subset=[TARGET_COLUMN]).reset_index(drop=True)\n",
    "\n",
    "target_candidates = [c for c in df.columns if c in [TARGET_COLUMN]]\n",
    "target_col = target_candidates[0]\n",
    "\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c != target_col]\n",
    "\n",
    "X = df_clean[numeric_cols]\n",
    "y = df_clean[target_col]\n",
    "\n",
    "# --- Ensure no NaNs before splitting: impute with column means ---\n",
    "X = X.replace([np.inf, -np.inf], np.nan)  # just in case\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=(1.0 - TRAIN_RATIO), random_state=42\n",
    ")\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d751d4",
   "metadata": {},
   "source": [
    "Run Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236954ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = lin_reg.predict(X_train)\n",
    "y_test_pred = lin_reg.predict(X_test)\n",
    "\n",
    "train_mse, train_rmse, train_mae, train_r2 = regression_metrics(y_train, y_train_pred)\n",
    "test_mse, test_rmse, test_mae, test_r2 = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"=== Train metrics ===\")\n",
    "print(f\"MSE : {train_mse:.4f}\")\n",
    "print(f\"RMSE: {train_rmse:.4f}\")\n",
    "print(f\"MAE : {train_mae:.4f}\")\n",
    "print(f\"R²  : {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=== Test metrics ===\")\n",
    "print(f\"MSE : {test_mse:.4f}\")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"MAE : {test_mae:.4f}\")\n",
    "print(f\"R²  : {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ee4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test.values, y_test_pred, n_points=360)\n",
    "\n",
    "print(len(y_pred_test_inv))\n",
    "print(len(y_test_inv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
